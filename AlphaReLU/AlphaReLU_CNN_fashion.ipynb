{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 1, 28, 28])\n",
      "torch.Size([100, 8, 28, 28])\n",
      "torch.Size([100, 16, 28, 28])\n",
      "torch.Size([100, 32, 28, 28])\n",
      "torch.Size([100, 25088])\n",
      "torch.Size([100, 10])\n",
      "torch.Size([100, 1, 28, 28])\n",
      "torch.Size([100, 8, 28, 28])\n",
      "torch.Size([100, 16, 28, 28])\n",
      "torch.Size([100, 32, 28, 28])\n",
      "torch.Size([100, 25088])\n",
      "torch.Size([100, 10])\n",
      "torch.Size([100, 1, 28, 28])\n",
      "torch.Size([100, 8, 28, 28])\n",
      "torch.Size([100, 16, 28, 28])\n",
      "torch.Size([100, 32, 28, 28])\n",
      "torch.Size([100, 25088])\n",
      "torch.Size([100, 10])\n",
      "torch.Size([100, 1, 28, 28])\n",
      "torch.Size([100, 8, 28, 28])\n",
      "torch.Size([100, 16, 28, 28])\n",
      "torch.Size([100, 32, 28, 28])\n",
      "torch.Size([100, 25088])\n",
      "torch.Size([100, 10])\n",
      "torch.Size([100, 1, 28, 28])\n",
      "torch.Size([100, 8, 28, 28])\n",
      "torch.Size([100, 16, 28, 28])\n",
      "torch.Size([100, 32, 28, 28])\n",
      "torch.Size([100, 25088])\n",
      "torch.Size([100, 10])\n",
      "torch.Size([100, 1, 28, 28])\n",
      "torch.Size([100, 8, 28, 28])\n",
      "torch.Size([100, 16, 28, 28])\n",
      "torch.Size([100, 32, 28, 28])\n",
      "torch.Size([100, 25088])\n",
      "torch.Size([100, 10])\n",
      "torch.Size([100, 1, 28, 28])\n",
      "torch.Size([100, 8, 28, 28])\n",
      "torch.Size([100, 16, 28, 28])\n",
      "torch.Size([100, 32, 28, 28])\n",
      "torch.Size([100, 25088])\n",
      "torch.Size([100, 10])\n",
      "torch.Size([100, 1, 28, 28])\n",
      "torch.Size([100, 8, 28, 28])\n",
      "torch.Size([100, 16, 28, 28])\n",
      "torch.Size([100, 32, 28, 28])\n",
      "torch.Size([100, 25088])\n",
      "torch.Size([100, 10])\n",
      "torch.Size([100, 1, 28, 28])\n",
      "torch.Size([100, 8, 28, 28])\n",
      "torch.Size([100, 16, 28, 28])\n",
      "torch.Size([100, 32, 28, 28])\n",
      "torch.Size([100, 25088])\n",
      "torch.Size([100, 10])\n",
      "torch.Size([100, 1, 28, 28])\n",
      "torch.Size([100, 8, 28, 28])\n",
      "torch.Size([100, 16, 28, 28])\n",
      "torch.Size([100, 32, 28, 28])\n",
      "torch.Size([100, 25088])\n",
      "torch.Size([100, 10])\n",
      "torch.Size([100, 1, 28, 28])\n",
      "torch.Size([100, 8, 28, 28])\n",
      "torch.Size([100, 16, 28, 28])\n",
      "torch.Size([100, 32, 28, 28])\n",
      "torch.Size([100, 25088])\n",
      "torch.Size([100, 10])\n",
      "torch.Size([100, 1, 28, 28])\n",
      "torch.Size([100, 8, 28, 28])\n",
      "torch.Size([100, 16, 28, 28])\n",
      "torch.Size([100, 32, 28, 28])\n",
      "torch.Size([100, 25088])\n",
      "torch.Size([100, 10])\n",
      "torch.Size([100, 1, 28, 28])\n",
      "torch.Size([100, 8, 28, 28])\n",
      "torch.Size([100, 16, 28, 28])\n",
      "torch.Size([100, 32, 28, 28])\n",
      "torch.Size([100, 25088])\n",
      "torch.Size([100, 10])\n",
      "torch.Size([100, 1, 28, 28])\n",
      "torch.Size([100, 8, 28, 28])\n",
      "torch.Size([100, 16, 28, 28])\n",
      "torch.Size([100, 32, 28, 28])\n",
      "torch.Size([100, 25088])\n",
      "torch.Size([100, 10])\n",
      "torch.Size([100, 1, 28, 28])\n",
      "torch.Size([100, 8, 28, 28])\n",
      "torch.Size([100, 16, 28, 28])\n",
      "torch.Size([100, 32, 28, 28])\n",
      "torch.Size([100, 25088])\n",
      "torch.Size([100, 10])\n",
      "torch.Size([100, 1, 28, 28])\n",
      "torch.Size([100, 8, 28, 28])\n",
      "torch.Size([100, 16, 28, 28])\n",
      "torch.Size([100, 32, 28, 28])\n",
      "torch.Size([100, 25088])\n",
      "torch.Size([100, 10])\n",
      "torch.Size([100, 1, 28, 28])\n",
      "torch.Size([100, 8, 28, 28])\n",
      "torch.Size([100, 16, 28, 28])\n",
      "torch.Size([100, 32, 28, 28])\n",
      "torch.Size([100, 25088])\n",
      "torch.Size([100, 10])\n",
      "torch.Size([100, 1, 28, 28])\n",
      "torch.Size([100, 8, 28, 28])\n",
      "torch.Size([100, 16, 28, 28])\n",
      "torch.Size([100, 32, 28, 28])\n",
      "torch.Size([100, 25088])\n",
      "torch.Size([100, 10])\n",
      "torch.Size([100, 1, 28, 28])\n",
      "torch.Size([100, 8, 28, 28])\n",
      "torch.Size([100, 16, 28, 28])\n",
      "torch.Size([100, 32, 28, 28])\n",
      "torch.Size([100, 25088])\n",
      "torch.Size([100, 10])\n",
      "torch.Size([100, 1, 28, 28])\n",
      "torch.Size([100, 8, 28, 28])\n",
      "torch.Size([100, 16, 28, 28])\n",
      "torch.Size([100, 32, 28, 28])\n",
      "torch.Size([100, 25088])\n",
      "torch.Size([100, 10])\n",
      "torch.Size([100, 1, 28, 28])\n",
      "torch.Size([100, 8, 28, 28])\n",
      "torch.Size([100, 16, 28, 28])\n",
      "torch.Size([100, 32, 28, 28])\n",
      "torch.Size([100, 25088])\n",
      "torch.Size([100, 10])\n",
      "torch.Size([100, 1, 28, 28])\n",
      "torch.Size([100, 8, 28, 28])\n",
      "torch.Size([100, 16, 28, 28])\n",
      "torch.Size([100, 32, 28, 28])\n",
      "torch.Size([100, 25088])\n",
      "torch.Size([100, 10])\n",
      "torch.Size([100, 1, 28, 28])\n",
      "torch.Size([100, 8, 28, 28])\n",
      "torch.Size([100, 16, 28, 28])\n",
      "torch.Size([100, 32, 28, 28])\n",
      "torch.Size([100, 25088])\n",
      "torch.Size([100, 10])\n",
      "torch.Size([100, 1, 28, 28])\n",
      "torch.Size([100, 8, 28, 28])\n",
      "torch.Size([100, 16, 28, 28])\n",
      "torch.Size([100, 32, 28, 28])\n",
      "torch.Size([100, 25088])\n",
      "torch.Size([100, 10])\n",
      "torch.Size([100, 1, 28, 28])\n",
      "torch.Size([100, 8, 28, 28])\n",
      "torch.Size([100, 16, 28, 28])\n",
      "torch.Size([100, 32, 28, 28])\n",
      "torch.Size([100, 25088])\n",
      "torch.Size([100, 10])\n",
      "torch.Size([100, 1, 28, 28])\n",
      "torch.Size([100, 8, 28, 28])\n",
      "torch.Size([100, 16, 28, 28])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 123\u001b[0m\n\u001b[1;32m    120\u001b[0m labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    122\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m--> 123\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    124\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m    126\u001b[0m \u001b[38;5;66;03m# Backprop and optimize\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[5], line 100\u001b[0m, in \u001b[0;36mConvNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     98\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer2(out)\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28mprint\u001b[39m(out\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m--> 100\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer3\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28mprint\u001b[39m(out\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m    102\u001b[0m out \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mview(out\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 463\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    456\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    457\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    458\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 459\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# config device\n",
    "device = torch.device('cpu')\n",
    "\n",
    "for Lambda in [0.5, 0.7]:\n",
    "    # hyper-parameters\n",
    "    num_epochs = 3\n",
    "    num_classes = 10\n",
    "    batch_size = 100\n",
    "    learning_rate = 0.001\n",
    "\n",
    "    #alphas1/2/3 is 0 iff linear, 1 iff ReLU\n",
    "    alpha1=0.5\n",
    "    alpha2=0.5\n",
    "    alpha3=0.5\n",
    "\n",
    "    #list for tracking alpha values\n",
    "    alpha_history = []\n",
    "    #Storing loss\n",
    "    losses = []\n",
    "\n",
    "    # FashionMNIST dataset\n",
    "    train_dataset = torchvision.datasets.FashionMNIST(\n",
    "        root='../../data/',\n",
    "        train=True,\n",
    "        transform=transforms.ToTensor(),\n",
    "        download=True\n",
    "    )\n",
    "    test_dataset = torchvision.datasets.FashionMNIST(\n",
    "        root='../../data/',\n",
    "        train=False,\n",
    "        transform=transforms.ToTensor()\n",
    "    )\n",
    "\n",
    "    # Data load\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        dataset=train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True\n",
    "    )\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        dataset=test_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "\n",
    "    class AlphaReLU(nn.Module):\n",
    "        def __init__(self, alpha=0.5):\n",
    "            super(AlphaReLU, self).__init__()\n",
    "            self.alpha = nn.Parameter(torch.tensor(alpha))\n",
    "\n",
    "        def forward(self, x):\n",
    "            return torch.relu(x) + self.alpha * torch.relu(-x)\n",
    "\n",
    "    # Customized AlphaCrossEntropyLoss\n",
    "    class AlphaCrossEntropyLoss(nn.Module):\n",
    "        def __init__(self, alpha1, alpha2, alpha3, weight=None, size_average=None, ignore_index=-100, reduce=None, reduction='mean', inplace=False):\n",
    "            super(AlphaCrossEntropyLoss, self).__init__()\n",
    "            self.loss_function = nn.CrossEntropyLoss(weight, size_average, ignore_index, reduce, reduction)\n",
    "            self.alpha1 = alpha1\n",
    "            self.alpha2 = alpha2\n",
    "            self.alpha3 = alpha3\n",
    "\n",
    "        def forward(self, outputs, targets):\n",
    "            loss = self.loss_function(outputs, targets) + Lambda*(abs(self.alpha1) + abs(self.alpha2) + abs(self.alpha3))\n",
    "            return loss\n",
    "\n",
    "    class ConvNet(nn.Module):\n",
    "        def __init__(self, num_classes=10, alpha1=0.5, alpha2=0.5, alpha3=0.5):\n",
    "            super(ConvNet, self).__init__()\n",
    "\n",
    "            self.layer1 = nn.Sequential(\n",
    "                nn.Conv2d(1, 8, kernel_size=5, stride=1, padding=2),\n",
    "                AlphaReLU(alpha1),\n",
    "            )\n",
    "            self.layer2 = nn.Sequential(\n",
    "                nn.Conv2d(8, 16, kernel_size=5, stride=1, padding=2),\n",
    "                AlphaReLU(alpha2),\n",
    "            )\n",
    "            self.layer3 = nn.Sequential(\n",
    "                nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),\n",
    "                AlphaReLU(alpha3),\n",
    "            )\n",
    "            self.fc = nn.Linear(25088, num_classes)\n",
    "\n",
    "        def forward(self, x):\n",
    "            print(x.shape)\n",
    "            out = self.layer1(x)\n",
    "            print(out.shape)\n",
    "            out = self.layer2(out)\n",
    "            print(out.shape)\n",
    "            out = self.layer3(out)\n",
    "            print(out.shape)\n",
    "            out = out.view(out.size(0), -1)\n",
    "            print(out.shape)\n",
    "            out = self.fc(out)\n",
    "            print(out.shape)\n",
    "            return out\n",
    "\n",
    "\n",
    "    model = ConvNet(num_classes, alpha1=alpha1, alpha2=alpha2, alpha3=alpha3).to(device)\n",
    "\n",
    "    criterion = AlphaCrossEntropyLoss(alpha1=alpha1, alpha2=alpha2, alpha3=alpha3)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Train\n",
    "    total_step = len(train_loader)\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (images, labels) in enumerate(train_loader):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backprop and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if (i + 1) % 100 == 0:\n",
    "                losses.append(loss.item())\n",
    "                alphas = {\n",
    "                    \"alpha1\": model.layer1[1].alpha.item(),\n",
    "                    \"alpha2\": model.layer2[1].alpha.item(),\n",
    "                    \"alpha3\": model.layer3[1].alpha.item()\n",
    "                            }\n",
    "                alpha_history.append(alphas)\n",
    "                print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n",
    "                    .format(epoch + 1, num_epochs, i + 1, total_step, loss.item()))\n",
    "\n",
    "    #Plot alpha history\n",
    "    # Access alpha values for each epoch\n",
    "    alpha1_values = [entry[\"alpha1\"] for entry in alpha_history]\n",
    "    alpha2_values = [entry[\"alpha2\"] for entry in alpha_history]\n",
    "    alpha3_values = [entry[\"alpha3\"] for entry in alpha_history]\n",
    "\n",
    "    epochs = range(1, len(alpha_history) + 1)\n",
    "\n",
    "    # Plotting alpha values over epochs\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(epochs, alpha1_values, label='Alpha 1')\n",
    "    plt.plot(epochs, alpha2_values, label='Alpha 2')\n",
    "    plt.plot(epochs, alpha3_values, label='Alpha 3')\n",
    "\n",
    "    plt.title('Alpha Values over Training')\n",
    "    plt.xlabel('Hundreds of steps')\n",
    "    plt.ylabel('Alpha Values')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig('Alphas_Values_over_training_{:.1f}.png'.format(Lambda))\n",
    "    plt.show()\n",
    "\n",
    "    from torchvision import utils\n",
    "    # Test the model\n",
    "    model.eval()  # eval mode (batchnorm uses moving mean/variance instead of mini-batch mean/variance)\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for images, labels in test_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        print('Model Accuracy on the 10000 test images: {} %'.format(100 * correct / total))\n",
    "        print('alpha1 = ', alpha1_values[-1], 'alpha2 = ', alpha2_values[-1], 'alpha3 = ', alpha3_values[-1])\n",
    "\n",
    "    #Plot loss\n",
    "    plt.plot(losses)\n",
    "    plt.title('Model Accuracy: {} %'.format(100 * correct / total))\n",
    "    plt.xlabel('Hundreds of Iterations')\n",
    "    plt.ylabel('Loss')\n",
    "    # Save the plot as a PNG file\n",
    "    plt.savefig('training_loss_plot_alpha_{:.1f}.png'.format(Lambda))\n",
    "    plt.show()\n",
    "\n",
    "    names= ['T-shirt/top', 'Trouser','Pullover','Dress','Coat','Sandal','Shirt','Sneaker','Bag','Ankle Boot'] \n",
    "    # Plot feature map (to adjust based on number of layer)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(10):  # Iterate through 10 images\n",
    "            images, labels = iter(test_loader).__next__()\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            print('Predicted Label for Image below is', names[predicted[i]])\n",
    "\n",
    "            image_list = [\n",
    "                images[i][0].cpu().numpy(),\n",
    "                model.layer1(images)[i][0].cpu().numpy(),\n",
    "                model.layer2(model.layer1(images))[i][0].cpu().numpy(),\n",
    "                model.layer3(model.layer2(model.layer1(images)))[i][0].cpu().numpy()\n",
    "            ]\n",
    "\n",
    "            # Display feature maps\n",
    "            for j, feature_map in enumerate(image_list):\n",
    "                plt.subplot(1, len(image_list), j + 1)\n",
    "                plt.imshow(feature_map, cmap='gray')  # Display grayscale image\n",
    "                if(j==0):\n",
    "                    plt.title('Input Image')\n",
    "                if(j>0): \n",
    "                    plt.title(f'Layer{j}')\n",
    "                plt.axis('off')\n",
    "            plt.savefig('Feature_map_alpha_{}_{:.1f}.png'.format(i,Lambda))\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset)\n",
    "len(test_dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
